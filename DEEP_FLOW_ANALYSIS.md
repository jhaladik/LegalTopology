# Deep Flow Analysis: Where We Use What and Why Synthesis Fails

## Current Architecture - Step by Step

### üéØ Phase 1: TENSION EXTRACTION (Excellent ‚úÖ)
**Location:** `query-decomposer-v2.ts`
**Model:** GPT-4o-mini
**Cost:** ~$0.001
**Time:** ~5 seconds

```javascript
// WHAT HAPPENS:
Input: "Soused 30 let chod√≠ p≈ôes pozemek..."
         ‚Üì
[GPT-4o-mini with sophisticated prompt]
         ‚Üì
Output: {
  tensions: ["formal_vs_factual", "time_creates_rights"],
  doctrines: ["vydr≈æen√≠", "vlastnick√©_pr√°vo"],
  absurdity_test: "Bez vydr≈æen√≠ by 30 let bylo irelevantn√≠"
}
```

**Why This Works Well:**
- Prompt is sophisticated and Czech-specific
- GPT-4o-mini is perfect for structured extraction
- Output is clean JSON with clear reasoning
- Absurdity test is brilliant validation

### üîÑ Phase 2: VECTORIZATION (Excellent ‚úÖ)
**Location:** `tension-vectorizer.ts`
**Model:** OpenAI text-embedding-3-large
**Cost:** ~$0.002
**Time:** ~2 seconds

```javascript
// VECTOR ARITHMETIC MAGIC:
For tension "formal_vs_factual":
  1. BASE = embed("form√°ln√≠ vlastnictv√≠ faktick√© u≈æ√≠v√°n√≠")
  2. ADD = embed("faktick√Ω stav") + embed("dlouhodob√© u≈æ√≠v√°n√≠") + ...
  3. SUBTRACT = embed("form√°ln√≠ n√°le≈æitosti") + embed("z√°pis v katastru") + ...
  4. RESULT = normalize(BASE + 0.3*ADD - 0.2*SUBTRACT) * 0.95

// MULTIPLE SEARCH STRATEGIES:
- Tension vectors (50% weight)
- Doctrine vectors (30% weight each)
- Keyword fallback (20% weight)
```

**Why This Works Well:**
- Vector arithmetic is innovative and correct
- Multiple parallel searches increase recall
- Weight balancing is sensible
- Source tracking helps understand results

### üîç Phase 3: VECTOR SEARCH (Good ‚úÖ)
**Location:** Cloudflare Vectorize
**Index:** czech-legal-topology
**Time:** ~10 seconds

```javascript
// WHAT WE SEARCH:
Vectorize.query(tension_vector_1, topK=20)  // Parallel
Vectorize.query(tension_vector_2, topK=20)  // Parallel
Vectorize.query(doctrine_vector_1, topK=10) // Parallel
Vectorize.query(doctrine_vector_2, topK=10) // Parallel
Vectorize.query(keyword_vector, topK=10)    // Parallel

// WHAT COMES BACK:
{
  matches: [
    {
      id: "statute_1095",
      score: 0.59,
      metadata: {
        type: "statute",
        section: "1095",
        text: "Vydr≈æen√≠ pr√°va odpov√≠daj√≠c√≠ho vƒõcn√©mu b≈ôemenu...",
        weight: 1.0
      }
    },
    {
      id: "case_22_Cdo_2570_98",
      score: 0.53,
      metadata: {
        type: "judicial",
        case_id: "22 Cdo 2570/98",
        court: "Nejvy≈°≈°√≠ soud",
        text: "[FULL TEXT - often 5000+ chars]",
        weight: 2.64,
        sections_referenced: ["1089", "1090"],
        pravni_veta: "[legal principle]"
      }
    }
  ]
}
```

**Current Problems:**
- ‚ùå **Score combination is naive** - just weighted average
- ‚ùå **No semantic deduplication** - same concept from multiple sources
- ‚ùå **Lost context** - we know WHY we searched (tension) but don't use it

### üìù Phase 4: SYNTHESIS (Poor ‚ùå)
**Location:** `synthesize-multi-v2.ts`
**Model:** GPT-4o
**Cost:** ~$0.15
**Time:** ~60 seconds

## THE CORE PROBLEM

### What We Send to GPT-4o:

```markdown
TOPOLOGICK√Å ANAL√ùZA:
Napƒõt√≠: formal_vs_factual (95%)
[... more tensions ...]

RELEVANTN√ç PR√ÅVN√ç √öPRAVA:
¬ß1095: Vydr≈æen√≠ pr√°va... [200 chars truncated]
¬ß1096: Dr≈æba... [200 chars truncated]
[... 8 more statutes ...]

JUDIKATURA:
22 Cdo 2570/98: [300 chars truncated]
5 C 161/2023: [300 chars truncated]
[... 3 more cases ...]

Poskytni anal√Ωzu skrze napƒõt√≠...
```

### The Critical Issues:

1. **üìö CONTEXT TRUNCATION**
   - Statutes: Only 200 chars (losing critical details)
   - Cases: Only 300 chars (losing legal reasoning)
   - Original full text in vector DB: 1000-5000 chars

2. **üîó LOST CONNECTIONS**
   - We know ¬ß1095 was found via "vydr≈æen√≠" doctrine
   - We know it relates to "formal_vs_factual" tension
   - But we don't tell GPT-4o these connections!

3. **üéØ NO RELEVANCE EXPLANATION**
   - GPT-4o doesn't know WHY each statute/case was selected
   - It has to guess the relevance from truncated text

4. **üí≠ GENERIC ANALYSIS**
   - The output reads like a law school essay
   - Not specific to the actual statutes/cases found
   - Could be written without the search results

## WHY SYNTHESIS FAILS - Detailed

### Example of Information Loss:

**What's in Vectorize:**
```json
{
  "case_id": "22 Cdo 2570/98",
  "text": "Nejvy≈°≈°√≠ soud rozhodl, ≈æe pro vydr≈æen√≠ pr√°va odpov√≠daj√≠c√≠ho vƒõcn√©mu b≈ôemenu je t≈ôeba, aby dr≈æitel vykon√°val pr√°vo po celou vydr≈æec√≠ dobu v dobr√© v√≠≈ôe, pokojnƒõ, nep≈ôeru≈°enƒõ a aby ≈°lo o dr≈æbu kvalifikovanou. Pouh√© strpƒõn√≠ pr≈Øchodu sousedem bez aktivn√≠ho br√°nƒõn√≠ nen√≠ dostaƒçuj√≠c√≠ pro vznik pr√°va cesty vydr≈æen√≠m. Je nutn√©, aby dr≈æba byla vykon√°v√°na jako pr√°vo, nikoliv z pouh√© shov√≠vavosti vlastn√≠ka pozemku...",
  "pravni_veta": "Pro vydr≈æen√≠ slu≈æebnosti cesty nestaƒç√≠ pouh√° tolerance vlastn√≠ka",
  "sections_referenced": ["1089", "1090", "1095"]
}
```

**What GPT-4o receives:**
```
22 Cdo 2570/98: Nejvy≈°≈°√≠ soud rozhodl, ≈æe pro vydr≈æen√≠ pr√°va odpov√≠daj√≠c√≠ho vƒõcn√©mu b≈ôemenu je t≈ôeba, aby dr≈æitel vykon√°val pr√°vo po celou vydr≈æec√≠ dobu v dobr√© v√≠≈ôe, pokojnƒõ, nep≈ôeru≈°...
```

**Lost Information:**
- The crucial principle about "pouh√© strpƒõn√≠" (mere tolerance)
- The distinction between right and permission
- The legal principle (pravni_veta)
- Which sections this case interprets

## SOLUTION PROPOSALS

### Solution 1: Better Context Packaging
```javascript
// Instead of truncating, send structured context:
const caseContext = {
  case_id: "22 Cdo 2570/98",
  relevance_to_tension: "formal_vs_factual",
  found_via: ["vydr≈æen√≠ doctrine", "tension vector"],
  score: 0.53,
  weight: 2.64,
  legal_principle: "Pro vydr≈æen√≠ slu≈æebnosti nestaƒç√≠ pouh√° tolerance",
  key_holding: "Mus√≠ b√Ωt vykon√°v√°no jako pr√°vo, ne ze shov√≠vavosti",
  sections_interpreted: ["1089", "1090", "1095"],
  full_text_sample: "[first 500 chars of reasoning]"
}
```

### Solution 2: Two-Stage Synthesis
```javascript
// STAGE 1: Focused extraction per document
for each relevant_document:
  prompt = `This document was found because it relates to tension: ${tension}
            Extract the specific holding that resolves this tension:`
  extraction = GPT-4o-mini(document.full_text, prompt)

// STAGE 2: Synthesis with extractions
synthesis_prompt = `Based on these specific holdings:
  - For tension "formal_vs_factual": [extracted holdings]
  - For tension "time_creates_rights": [extracted holdings]
  Synthesize strategic analysis...`
```

### Solution 3: Tension-Specific Retrieval
```javascript
// Don't just search and combine - search WITH PURPOSE
for each tension:
  // Get documents specifically for THIS tension
  results = searchForTensionResolution(tension)

  // Extract tension-specific insights
  insights = extractTensionInsights(results, tension)

  // Build tension-specific analysis
  analysis[tension] = synthesizeTensionAnalysis(insights)
```

### Solution 4: Preserve Legal Reasoning Chain
```javascript
const reasoningChain = {
  step1_tension: "formal_vs_factual",
  step2_doctrine: "vydr≈æen√≠",
  step3_statutes: ["¬ß1089 - defines vydr≈æen√≠", "¬ß1095 - special rules for easements"],
  step4_cases: ["22 Cdo - requires active exercise, not tolerance"],
  step5_application: "Here, 30 years of use likely qualifies as active exercise",
  step6_conclusion: "Vydr≈æen√≠ likely established, owner cannot fence"
}
```

## RECOMMENDED IMPLEMENTATION

### Priority 1: Fix Synthesis Context (Quick Win)
```javascript
// In synthesize-multi-v2.ts, change:
const statuteContext = statutes.map(s =>
  `${s.section}: ${s.text.substring(0, 1000)}  // Increase from 200
   [Found via: ${s.sources.join(', ')}]
   [Relates to tension: ${getTensionRelevance(s, tensions)}]`
)

const caseContext = cases.map(c =>
  `${c.case_id}:
   Legal Principle: ${c.pravni_veta || 'N/A'}
   Key Holding: ${c.text.substring(0, 1000)}  // Increase from 300
   Interprets: ¬ß¬ß${c.sections_referenced.join(', ')}
   [Weight: ${c.weight}, Relevance: ${c.relevance}]
   [Resolves tension: ${getTensionRelevance(c, tensions)}]`
)
```

### Priority 2: Selective Deep Dive
```javascript
// For top 3 most relevant documents, include MORE context
const topStatutes = statutes.slice(0, 3).map(s => ({
  ...s,
  full_analysis: extractStatuteInsights(s.full_text, tensions)
}))

const topCases = cases.slice(0, 3).map(c => ({
  ...c,
  full_analysis: extractCaseHolding(c.full_text, tensions)
}))
```

### Priority 3: Feedback Loop
```javascript
// Track which sources actually contributed to final analysis
const synthesisMetrics = {
  statutes_cited_in_analysis: [],
  cases_cited_in_analysis: [],
  tensions_resolved: [],
  unused_search_results: []
}
// Use this to improve search strategy
```

## CONCLUSION

The system's first 3 phases are excellent:
1. ‚úÖ Tension extraction - sophisticated and accurate
2. ‚úÖ Vectorization - innovative vector arithmetic
3. ‚úÖ Search - multi-strategy with good recall

But Phase 4 (Synthesis) fails because:
- ‚ùå Truncates critical legal reasoning
- ‚ùå Loses connection between documents and tensions
- ‚ùå Doesn't explain WHY documents are relevant
- ‚ùå Generic analysis instead of specific application

**The fix:** Don't truncate context, preserve reasoning chains, and make synthesis tension-specific rather than generic.